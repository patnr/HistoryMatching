{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d2f2c2b",
   "metadata": {},
   "source": [
    "# History matching and optimisation with ensembles – an interactive tutorial\n",
    "\n",
    "Copyright Patrick N. Raanes, NORCE, 2020.\n",
    "\n",
    "This is a self-contained tutorial on history matching (HM) and optimisation using ensemble methods.\n",
    "- If you \"run all\" then you will burn through it in 5 min.\n",
    "- For a more detailed reading, expect to spend around 5 hours.\n",
    "- The code emphasises simplicity, not generality.\n",
    "- Do not hesitate to file issues on\n",
    "  [GitHub](https://github.com/patnr/HistoryMatching),\n",
    "  or submit pull requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27a5bec",
   "metadata": {},
   "source": [
    "## Python in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e03c88",
   "metadata": {},
   "source": [
    "**Jupyter notebooks** combine **cells/blocks** of code (Python) and text (markdown).\n",
    "\n",
    "For example, try to **edit** the cell below to insert your name, and then **run** it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Batman\"\n",
    "print(\"Hello world! I'm\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e38ec5",
   "metadata": {},
   "source": [
    "Next, as an exercise, try to **insert** a new cell here, and compute `23/3`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf8c4f",
   "metadata": {},
   "source": [
    "You will likely be more efficient if you know these **keyboard shortcuts**:\n",
    "\n",
    "| Navigate                      |    | Edit              |    | Exit           |    | Run & advance                     |\n",
    "| -------------                 | -- | ----------------- | -- | --------       | -- | -------------                     |\n",
    "| <kbd>↓</kbd> and <kbd>↑</kbd> |    | <kbd>Enter</kbd>  |    | <kbd>Esc</kbd> |    | <kbd>Shift</kbd>+<kbd>Enter</kbd> |\n",
    "\n",
    "When you open a notebook it starts a **session (interpreter/kernel/runtime)** of\n",
    "Python in the background.  All of the code cells (in a given notebook) are connected\n",
    "(share kernel and thus share variables, functions, and classes).  Thus, the **order**\n",
    "in which you run the cells matters.  One thing you must know is how to **restart** the\n",
    "session, so that you can start over. Try to locate this option via the menu bar at the\n",
    "top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8606a7c9",
   "metadata": {},
   "source": [
    "If you're on **Google Colab**, run the cell below to install the requirements.\n",
    "Otherwise (and assuming you have done the installation described in the README),\n",
    "you can skip/delete this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebeb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote = \"https://raw.githubusercontent.com/patnr/HistoryMatching\"\n",
    "!wget -qO- {remote}/master/colab_bootstrap.sh | bash -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730a86c",
   "metadata": {},
   "source": [
    "There is a huge amount of libraries available in **Python**,\n",
    "including the popular `numpy (np)` and `matplotlib/pyplot (mpl/plt)` packages.\n",
    "Try them out by running in the next few cells following,\n",
    "which illustrates some algebra using syntax reminiscent of Matlab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea62e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tools import mpl_setup\n",
    "mpl_setup.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a792ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use numpy arrays for vectors, matrices. Examples:\n",
    "a  = np.arange(10)  # OR: np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "Id = 2*np.eye(10)   # OR: np.diag(2*np.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d79d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Indexing examples:\")\n",
    "print(\"a         =\", a)\n",
    "print(\"a[3]      =\", a[3])\n",
    "print(\"a[0:3]    =\", a[0:3])\n",
    "print(\"a[:3]     =\", a[:3])\n",
    "print(\"a[3:]     =\", a[3:])\n",
    "print(\"a[-1]     =\", a[-1])\n",
    "print(\"Id[:3,:3] =\", Id[:3, :3], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd2d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear algebra examples:\")\n",
    "print(\"100 + a =\", 100+a)\n",
    "print(\"Id @ a  =\", Id@a)\n",
    "print(\"Id * a  =\", Id*a, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03421900",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Plotting example\")\n",
    "plt.ylabel(\"$i \\\\, x^2$\")\n",
    "for i in range(4):\n",
    "    plt.plot(i * a**2, label=\"i = %d\" % i)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c1164",
   "metadata": {},
   "source": [
    "Run the following cells to import yet more tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d5142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy.random as rnd\n",
    "import scipy.linalg as sla\n",
    "from matplotlib.ticker import LogLocator\n",
    "from mpl_tools.place import freshfig\n",
    "from numpy import sqrt\n",
    "from struct_tools import DotDict as Dict\n",
    "from tqdm.auto import tqdm as progbar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e1584",
   "metadata": {},
   "source": [
    "## Problem case (simulator, truth, obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e36d14",
   "metadata": {},
   "source": [
    "For exact reproducibility of our problem/case, we set the random generator seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c9d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = rnd.seed(4)  # very easy\n",
    "# seed = rnd.seed(5)  # hard\n",
    "# seed = rnd.seed(6)  # very easy\n",
    "# seed = rnd.seed(7)  # easy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d45f5e",
   "metadata": {},
   "source": [
    "Our reservoir simulator takes up about 100 lines of python code. This may seem\n",
    "outrageously simple, but serves the purpose of *illustrating* the main features of\n",
    "the history matching process. Indeed, we do not detail the simulator code here, but\n",
    "simply import it from the accompanying python modules, together with the associated\n",
    "plot functionality, the (geostatistical) random field generator, and some linear\n",
    "algebra. Hence our focus and code will be of aspects directly related to the history\n",
    "matching and optimisation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import TPFA_ResSim as simulator\n",
    "import tools.plotting as plotting\n",
    "import tools.localization as loc\n",
    "from tools import geostat, utils\n",
    "from tools.utils import center, ens_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5cc2a0",
   "metadata": {},
   "source": [
    "In short, the model is a 2D, two-phase, immiscible, incompressible simulator using\n",
    "two-point flux approximation (TPFA) discretisation. It was translated from the Matlab\n",
    "code here http://folk.ntnu.no/andreas/papers/ResSimMatlab.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56947dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simulator.ResSim(Nx=20, Ny=20, Lx=2, Ly=1)\n",
    "\n",
    "# Also init plotting module\n",
    "plotting.single.model = model\n",
    "plotting.single.coord_type = \"absolute\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6440a86e",
   "metadata": {},
   "source": [
    "The following declares some data containers to help us keep organised.\n",
    "The names have all been shortened to 4 characters, but this is just\n",
    "to obtain more convenient code alignment and readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e09c20",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Permeability\n",
    "perm = Dict()\n",
    "\n",
    "# Production\n",
    "prod = Dict(\n",
    "    past = Dict(),\n",
    "    futr = Dict(),\n",
    ")\n",
    "\n",
    "# Water saturation\n",
    "wsat = Dict(\n",
    "    init = Dict(),\n",
    "    past = Dict(),\n",
    "    futr = Dict(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80aee84",
   "metadata": {},
   "source": [
    "Technical note: This data hierarchy is convienient in *this* notebook/script,\n",
    "especially for plotting purposes. For example, we can with ease refer to\n",
    "`wsat.past.Truth` and `wsat.past.Prior`. The former will be a numpy array of shape\n",
    "`(nTime, model.Nxy)` and the latter will have shape `(N, nTime, model.Nxy)` where\n",
    "`N` is the size of the ensemble. However, in other implementations, different choices\n",
    "for the data structure may be more convenient, e.g. where the different types of\n",
    "the unknowns are merely concatenated along the last axis, rather than being kept in\n",
    "separate dicts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d555112b",
   "metadata": {},
   "source": [
    "#### The unknown: permeability\n",
    "We will estimate the log permeability field.  We *parameterize* the permeability,\n",
    "meaning that they are defined via some transform (function), which becomes part of the\n",
    "forward model. We term the parameterized permeability fields \"pre-permeability\".\n",
    "\n",
    "*If* we use the exponential, then we will we working with log-permeabilities.\n",
    "In any case, the transform should be chosen so that the parameterized permeabilities\n",
    "are suited for ensemble methods, i.e. are distributed as a Gaussian.  But this\n",
    "consideration must be weighted against the fact that that nonlinearity (another\n",
    "difficulty for ensemble methods) in the transform might add to the nonlinearity of\n",
    "the total/composite forward model.\n",
    "\n",
    "Since this is a synthetic case, we can freely choose *both* the distribution of the\n",
    "parameterized permeabilities, *and* the transform.  Here we use Gaussian fields, and a\n",
    "almost-exponential function (to make the problem slightly trickier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194cb128",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def sample_prior_perm(N):\n",
    "    lperms = geostat.gaussian_fields(model.mesh, N, r=0.8)\n",
    "    return lperms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3dc094",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def perm_transf(x):\n",
    "    return .1 + np.exp(5*x)\n",
    "    # return 1000*np.exp(3*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feffca2",
   "metadata": {},
   "source": [
    "Also configure plot parameters suitable for pre-perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f6ab7e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "plotting.styles[\"pperm\"][\"levels\"] = np.linspace(-4, 4, 21)\n",
    "plotting.styles[\"pperm\"][\"cticks\"] = np.arange(-4, 4+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1486212",
   "metadata": {},
   "source": [
    "For any type of parameter, one typically has to write a \"setter\" function that takes\n",
    "the vector of parameter parameter values, and applies it to the specific model\n",
    "implementation. We could merge this functionality with `perm_transf` (and indeed the\n",
    "\"setter\" function is also part of the composite forward model) but it is convenient to\n",
    "separate these implementation specifics from the mathematics going on in\n",
    "`perm_transf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea31b375",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def set_perm(model, log_perm_array):\n",
    "    \"\"\"Set perm. in model code (both x and y components).\"\"\"\n",
    "    p = perm_transf(log_perm_array)\n",
    "    p = p.reshape(model.shape)\n",
    "    model.Gridded.K = np.stack([p, p])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed92a786",
   "metadata": {},
   "source": [
    "Now we are in position to sample the permeability of the (synthetic) truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b420aeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm.Truth = sample_prior_perm(1)\n",
    "set_perm(model, perm.Truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42f3a85",
   "metadata": {},
   "source": [
    "#### Wells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2944027d",
   "metadata": {},
   "source": [
    "In this model, wells are represented simply by point **sources** and **sinks**.\n",
    "This is of course incredibly basic and not realistic, but works for our purposes.\n",
    "So all we need to specify is their placement and flux (which we will not vary in time).\n",
    "The code below generates the coordinates of the 4 corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0ba141",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_xy = np.dstack(np.meshgrid(\n",
    "    np.array([.12, .87]) * model.Lx,\n",
    "    np.array([.12, .87]) * model.Ly\n",
    ")).reshape((-1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d115f07a",
   "metadata": {},
   "source": [
    "Since the **boundary conditions** are Dirichlet, specifying *zero flux*, and the fluid\n",
    "is incompressible, the total of the source terms must equal that of the sinks. This is\n",
    "ensured by the `config_wells` function used below. We will not vary the rates in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8cfb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "nProd = len(prod_xy)\n",
    "model.config_wells(\n",
    "    inj_xy = [[0.50*model.Lx, 0.50*model.Ly]],\n",
    "    inj_rates = [[1]],\n",
    "    prod_xy = prod_xy,\n",
    "    prod_rates = np.ones((nProd, 1)) / nProd,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b541a",
   "metadata": {},
   "source": [
    "#### Plot\n",
    "Let's take a moment to visualize the (true) model permeability field,\n",
    "and the well locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e01f7c0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig, ax = freshfig(\"True perm. field\", figsize=(1.5, 1), rel=1)\n",
    "# plotting.field(ax, perm.Truth, \"pperm\")\n",
    "plotting.field(ax, perm_transf(perm.Truth),\n",
    "               locator=LogLocator(), wells=True, colorbar=True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c4ef89",
   "metadata": {},
   "source": [
    "#### Observation operator\n",
    "The data will consist in the water saturation of at the well locations, i.e. of the\n",
    "production. I.e. there is no well model. It should be pointed out, however, that\n",
    "ensemble methods technically support observation models of any complexity, though your\n",
    "accuracy mileage may vary (again, depending on the incurred nonlinearity and\n",
    "non-Gaussianity). Furthermore, it is also no problem to include time-dependence in the\n",
    "observation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22b562",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "obs_inds = model.xy2ind(*model.prod_xy.T)\n",
    "def obs_model(water_sat):\n",
    "    return water_sat[obs_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d0e19",
   "metadata": {},
   "source": [
    "#### Simulation\n",
    "The following generates the synthetic truth evolution and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16872a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1\n",
    "dt = 0.025\n",
    "nTime = round(T/dt)\n",
    "wsat.init.Truth = np.zeros(model.Nxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a07392",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsat.past.Truth = simulator.recurse(model.time_stepper(dt), nTime, wsat.init.Truth)\n",
    "prod.past.Truth = np.array([obs_model(x) for x in wsat.past.Truth[1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b5add8",
   "metadata": {},
   "source": [
    "#### Animation\n",
    "Run the code cells below to get an animation of the oil saturation evolution.\n",
    "Injection/production wells are marked with triangles pointing down/up.\n",
    "The (untransformed) pre-perm field is plotted, rather than the actual permeability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f833e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "animation = plotting.single.anim(None, wsat.past.Truth, prod.past.Truth);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6f910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: can take up to a minute to appear\n",
    "animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9a851",
   "metadata": {},
   "source": [
    "#### Noisy obs\n",
    "In reality, observations are never perfect. To emulate this, we corrupt the\n",
    "observations by adding a bit of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ac156c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "prod.past.Noisy = prod.past.Truth.copy()\n",
    "R = 1e-3 * np.eye(nProd)\n",
    "for iT in range(nTime):\n",
    "    prod.past.Noisy[iT] += sqrt(R) @ rnd.randn(nProd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad0c37a",
   "metadata": {},
   "source": [
    "Plot of observations (and their noise):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = freshfig(\"Observations\", figsize=(2, .7), rel=True)\n",
    "plotting.single.production(ax, prod.past.Truth, prod.past.Noisy);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5780aefb",
   "metadata": {},
   "source": [
    "Note that several observations are above 1,\n",
    "which is \"unphysical\" or not physically \"realisable\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2622cc61",
   "metadata": {},
   "source": [
    "## Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec91e58",
   "metadata": {},
   "source": [
    "The prior ensemble is generated in the same manner as the (synthetic) truth, using the\n",
    "same mean and covariance.  Thus, the members are \"statistically indistinguishable\" to\n",
    "the truth. This assumption underlies ensemble methods.\n",
    "\n",
    "In practice, \"encoding\" prior information, from a range of experts, and prior\n",
    "information sources, in such a way that it is useful for decision making analyses (and\n",
    "history matching), is a formidable tecnnical task, typically involving multiple\n",
    "different types of modelling.  Nevertheless it is crucial, and must be performed with\n",
    "care."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75bcfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "perm.Prior = sample_prior_perm(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0969395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that field (before transformation) is Gaussian with (expected) mean 0 and variance 1.\n",
    "print(\"Prior mean:\", np.mean(perm.Prior))\n",
    "print(\"Prior var.:\", np.var(perm.Prior))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ebcaa1",
   "metadata": {},
   "source": [
    "#### Histogram\n",
    "Let us inspect the parameter values in the form of their histogram.\n",
    "Note that the histogram of the truth is simply counting the values of a single field,\n",
    "whereas the histogram of the ensemble counts the values of `N` fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30c1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = freshfig(\"Perm.\", figsize=(1.5, .7), rel=1)\n",
    "bins = np.linspace(*plotting.styles[\"pperm\"][\"levels\"][[0, -1]], 32)\n",
    "for label, perm_field in perm.items():\n",
    "    x = perm_field.ravel()\n",
    "    ax.hist(perm_transf(x),\n",
    "            perm_transf(bins),\n",
    "            # Divide counts by N to emulate `density=1` for log-scale.\n",
    "            weights=(np.ones_like(x)/N if label != \"Truth\" else None),\n",
    "            label=label, alpha=0.3)\n",
    "ax.set(xscale=\"log\", xlabel=\"Permeability\", ylabel=\"Count\")\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce481243",
   "metadata": {},
   "source": [
    "Since the x-scale is logarithmic, the prior's histogram should look Gaussian if\n",
    "`perm_transf` is purely exponential. By contrast, the historgram of the truth is from\n",
    "a single (spatially extensive) realisation, and therefore will contain significant\n",
    "sampling \"error\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4440d26",
   "metadata": {},
   "source": [
    "#### Field plots\n",
    "Below we can see some (pre-perm) realizations (members) from the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a04c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.fields(perm.Prior, \"pperm\", \"Prior\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa02f9",
   "metadata": {},
   "source": [
    "#### Variance/Spectrum\n",
    "In practice, of course, we would not be using an explicit `Cov` matrix when generating\n",
    "the prior ensemble, because it would be too large.  However, since this synthetic case\n",
    "in being made that way, let's inspect its spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5c24ad",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "U, svals, VT = sla.svd(perm.Prior)\n",
    "plotting.spectrum(svals, \"Prior cov.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cabb697",
   "metadata": {},
   "source": [
    "With our limited ensemble size, we see no clear cutoff index. In other words, we are\n",
    "not so fortunate that the prior is implicitly restricted to some subspace that is of\n",
    "lower rank than our ensemble. This is a very realistic situation, and indicates that\n",
    "localization (implemented further below) will be very beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8309b2c",
   "metadata": {},
   "source": [
    "## Forward model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099e15ca-9224-419e-8de4-08e2762ad545",
   "metadata": {},
   "source": [
    "In order to (begin to attempt to) solve the *inverse problem*,\n",
    "we first have to be able to solve the *forward problem*.\n",
    "Indeed, ensemble methods obtain observation-parameter sensitivities\n",
    "from the covariances of the ensemble run through the (\"forward\") model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc0b70-f0de-4ab2-aa41-e8f60cbe3265",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Function composition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bab18c0-b289-439c-93b4-b7e5b7fb59eb",
   "metadata": {},
   "source": [
    "The forward model is generally a composite function.\n",
    "In our simple case, it only consists of two steps:\n",
    "\n",
    "- The main work consists of running the reservoir simulator\n",
    "  for each realisation in the ensemble.\n",
    "- However, the simulator only inputs/outputs *state* variables,\n",
    "  so we also have to take the necessary steps to set the *parameter* values.\n",
    "\n",
    "This all has to be stitched together; this is not usually a pleasant task, though some\n",
    "tools like [ERT](https://github.com/equinor/ert) have made it a little easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9bc135",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def forward_model(parameters):\n",
    "    \"\"\"Forecast (composite) model for a *single* member (realisation).\"\"\"\n",
    "    wsat0, perm = parameters\n",
    "\n",
    "    # Set attribute params, w/o overwriting truth model\n",
    "    model_n = copy.deepcopy(model)\n",
    "    set_perm(model_n, perm)\n",
    "\n",
    "    # Run simulator\n",
    "    wsats = simulator.recurse(model_n.time_stepper(dt), nTime, wsat0, pbar=False)\n",
    "    prods = np.array([obs_model(x) for x in wsats[1:]])  # extract prod time series\n",
    "\n",
    "    # While we only reall need the state at the *final* time (for future predictions),\n",
    "    # for possible diagnostic purposes we emit the full time series of wsats.\n",
    "    return wsats, prods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720fa397-d9e8-456e-9d78-7e0e6ad80c40",
   "metadata": {},
   "source": [
    "Note that the input to `forward_model` should contain **not only** permeability\n",
    "fields, but **also** the state (i.e. time-dependent, prognostic) variable,\n",
    "i.e. water saturations.\n",
    "Why? Because further below we'll be \"restarting\" (running) the simulator\n",
    "from a later point in time (to generate future predictions) in which case\n",
    "the saturation fields (which is also among the outputs) will depend on the\n",
    "given permeability field, and hence vary from realisation to realisation.\n",
    "\n",
    "But for time 0, the saturations (in this case study) are not uncertain (unknown).\n",
    "We can express this 100% certainty by setting each saturation field\n",
    "equal to the *true* time-0 saturation (a constant field of 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec28cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsat.init.Prior = np.tile(wsat.init.Truth, (N, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86a6d5b-5789-4679-8aa6-452ab67d0a8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef173e62",
   "metadata": {},
   "source": [
    "A huge technical advantage of ensembel methods is that they are\n",
    "\"embarrasingly parallelizable\", because each member simulation\n",
    "is completely independent (requires no communication) from the others.\n",
    "Configure the number of CPUs to use in `ens_run`. Can set to an `int` or False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d028266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.nCPU = \"auto\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e2f9d0",
   "metadata": {},
   "source": [
    "Now that our forward model is ready, we can make prior estimates of the saturation\n",
    "evolution and production. This is interesting in and of itself and, as we'll see\n",
    "later, is part of the assimilation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e9602e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "(wsat.past.Prior,\n",
    " prod.past.Prior) = ens_run(forward_model, wsat.init.Prior, perm.Prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e4a98-6c66-4013-8bc0-1262a174ffce",
   "metadata": {},
   "source": [
    "#### Flattening the time dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc05b21-a9c4-4e8d-82b8-9c98b4af6a85",
   "metadata": {},
   "source": [
    "We have organised our ensemble data in 3D arrays,\n",
    "with *time* along the second-to-last axis.\n",
    "Ensemble methods have no notion of 3D arrays, however, so we need to\n",
    "be able to flatten the time and space dimensions (as well as to undo this).\n",
    "Providing we stick to a given array axis ordering,\n",
    "here is a convenient function for juggling the array shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840308a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def vect(x, undo=False):\n",
    "    \"\"\"Unravel/flatten the last two axes. Assumes axis `-2` has length `nTime`.\"\"\"\n",
    "    # Works both for ensemble (3D) and single-realisation (2D) arrays.\n",
    "    if undo:\n",
    "        *N, ab = x.shape\n",
    "        return x.reshape(N + [nTime, ab//nTime])\n",
    "    else:\n",
    "        *N, a, b = x.shape\n",
    "        return x.reshape(N + [a*b])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37227e8d",
   "metadata": {},
   "source": [
    "Similarly, we need to specify the observation error covariance matrix for the\n",
    "flattened observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28ddd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_obs_error_cov = sla.block_diag(*[R]*nTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e4105d",
   "metadata": {},
   "source": [
    "## Correlation study (*a-priori*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15e52e9-a99a-4027-b316-d2ef93e733c7",
   "metadata": {},
   "source": [
    "#### The mechanics of the Kalman gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc8b8c7-74d8-4bcb-922f-2671e7882909",
   "metadata": {},
   "source": [
    "The conditioning \"update\" of ensemble methods is often formulated in terms of a\n",
    "\"**Kalman gain**\" matrix, derived so as to achieve a variety of optimality properties\n",
    "(see e.g. [[Jaz70]](#Jaz70)):\n",
    "- in the linear-Gaussian case, to compute the correct posterior moments;\n",
    "- in the linear (not-necessarily-Gaussian) case, to compute the [BLUE/MMSE](https://en.wikipedia.org/wiki/Kalman_filter#Kalman_gain_derivation),\n",
    "  which is akin to achieving orthogonality of the posterior error and innovation;\n",
    "- in the non-linear, non-Gaussian case, the *ensemble* Kalman gain can be derived as\n",
    "  linear regression (with some tweaks) from the noisy obs. to the unknowns.\n",
    "\n",
    "Another way to look at it is to ask \"what does it do?\"\n",
    "Heuristically, this may be answered as follows:\n",
    "\n",
    "- It uses correlation coefficients to establish relationships between\n",
    "  observations and unknowns. For example, if there is no correlation,\n",
    "  the unknowns do not get updated.\n",
    "- It takes into account the \"intermingling\" of correlations. For example, two\n",
    "  measurements/observations that are highly correlated\n",
    "  (when including both prior uncertainty and observation noise)\n",
    "  will barely have more impact than either one alone.\n",
    "- It also takes into account the variables' variance,\n",
    "  and thereby the their relative uncertainties\n",
    "  (in fact linear least-squares regression is a straighforward\n",
    "  combination of variances and correlation coefficients).\n",
    "  For example, if two variables have equal correlation with an observation,\n",
    "  but one is more uncertain, that one will receive a larger update than the other.\n",
    "  Conversely, an observation with a larger variance will have less impact than an\n",
    "  observation with a smaller variance. Working with variances also means that\n",
    "  the physical units of the variables are inherently accounted for.\n",
    "\n",
    "In summary, it is useful to investigate the correlation relations of the ensemble,\n",
    "especially for the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3999ecbb-6d0b-4f03-ba70-79c3275ad0c1",
   "metadata": {},
   "source": [
    "#### Exploratory correlation plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac84bbd-e582-4e81-9819-33b6380697e7",
   "metadata": {},
   "source": [
    "The following plots a variety of different correlation fields. Each field may\n",
    "be seen as a single column (or row) of a larger (\"cross\")-covariance matrix,\n",
    "which would typically be too large for explicit computation or storage. The\n",
    "following solution, though, which computes the correlation fields \"on the\n",
    "fly\", should be viable for relatively large scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263bb107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available variable types\n",
    "prior_fields = {\n",
    "    \"Saturation\": lambda time: wsat.past.Prior[:, time],\n",
    "    \"Pre-perm\"  : lambda time: perm.Prior,\n",
    "}\n",
    "\n",
    "# Compute correlation field\n",
    "def corr_comp(N, Field, T, Point, t, x, y):\n",
    "    xy = model.sub2ind(x, y)\n",
    "    Point = prior_fields[Point](t)[:, xy]\n",
    "    Field = prior_fields[Field](T)\n",
    "    return utils.corr(Field[:N], Point[:N])\n",
    "\n",
    "# Register controls\n",
    "corr_comp.controls = dict(\n",
    "    N = (2, N),\n",
    "    Field = list(prior_fields),\n",
    "    T = (0, nTime),\n",
    "    Point = list(prior_fields),\n",
    "    t = (0, nTime),\n",
    "    x = (0, model.Nx-1),\n",
    "    y = (0, model.Ny-1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827491bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.field_console(corr_comp, \"corr\", argmax=True, wells=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc80d4c3",
   "metadata": {},
   "source": [
    "Use the interative control widgets to investigate the correlation structure.\n",
    "Answer the following questions. *NB*: the order matters!\n",
    "\n",
    "- Set the times as `T = t = 20`\n",
    "    and the variable kinds as `Field = Point = \"Saturation\"`.\n",
    "  - Move the point around (`x` and `y` sliders).\n",
    "  - Why is the star marker (showing the location of the maximum)\n",
    "    on top of the crosshairs?\n",
    "    <!-- Answer: Because the correlation the correlation of a variable with itself\n",
    "    is 1.00, which is the maximum possible correlation. This is a useful sanity check\n",
    "    on our correlation and plotting facilities. Use the zoom functionality if necessary\n",
    "    to assert exact superposition.\n",
    "    -->\n",
    "- Set `Field = \"Pre-perm\"`.\n",
    "  - Move `T` around. Why doesn't anything change?\n",
    "  - Set the ensemble size: `N=2`. How does the correlation field look? Why?\n",
    "    <!-- Answer: Only 2 colors, because 2 points always lie on a straight line -->\n",
    "- Now set `Field = \"Saturation\"`. Explain the major new and strange appearance.\n",
    "  <!-- Answer: Nan's and inf's at corners. Reason: for most realisations,\n",
    "  the saturation is (as of yet) constant there.\n",
    "  -->\n",
    "- Set `N=200`. Move the point to the center again.\n",
    "  - Set `T=0` How do the correlation fields look? Why?\n",
    "  - Set `t=T=1`. Gradually move `T=2,3,4, etc` (hint: use your arrow keys).\n",
    "    Explain the appearance of \"fronts\".\n",
    "  - Move `T=1,2,3, etc` using your arrow keys. Explain the appearance of \"fronts\".\n",
    "- Set `T=20`, `t=40`, and move the point to the location of one of the wells.\n",
    "  - Where is the maximum? And minimum? Does this make sense?\n",
    "  - Gradually increase `T`. How do the extrema move? Why?.\n",
    "- Set `T=40`. Note the location of the maximum. Now switch to `Field = \"Per-perm\"`.\n",
    "  Note that we are now investigate the correlation between\n",
    "  the unknowns and the observations.\n",
    "  - Where is the maximum now? Does it make sense?\n",
    "  - Gradually decrease `t` back down to `10`.\n",
    "    Describe and explain the change in the correlation field.\n",
    "    <!-- Answer: it weakens, but does not move a lot.\n",
    "    It weakens because the early production (saturation) is 100% anyway,\n",
    "    thus independent of the permeability fields.\n",
    "    -->\n",
    "  - Set `t=40` again. Explain the appearance of negative correlations on the opposite\n",
    "    side of where the point (`x` and `y`) is located.\n",
    "    <!-- Answer: The negative correlations arise because a low permeability on the\n",
    "    other side will make the injector pump more water in the direction of the point.\n",
    "    -->\n",
    "- Set `t=30` and `N=2`, then (use your arrow keys to) gradually increase `N`\n",
    "  to `20`. Do the (changes you observe in the) correlation fields inspire confidence?\n",
    "  Actually, that's a rhetorical question; the answer is clearly no.\n",
    "- Now try flipping between low and high values of `N`.\n",
    "  What do you think the tapering radius should be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74e2014",
   "metadata": {},
   "source": [
    "#### Location of correlation extrema\n",
    "In the preceding dashboard we could observe that the \"locations\" (defined as the\n",
    "location of the maximum) of the correlations (between a given well observation\n",
    "and the permeability field) moved in time. Let us trace these paths computationally.\n",
    "They will be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd345323",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_max_corr = np.zeros((nProd, nTime, 2))\n",
    "for i, xy_path in enumerate(xy_max_corr):\n",
    "    for time in range(6, nTime):\n",
    "        C = utils.corr(perm.Prior, prod.past.Prior[:, time, i])\n",
    "        xy_path[time] = model.ind2xy(np.argmax(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68afea1a",
   "metadata": {},
   "source": [
    "In general, minima might be just as relevant as maxima.\n",
    "In our case, though, it's a safe bet to focus on the maxima, which also avoids\n",
    "the danger of jumping from one case to another in case of weak correlations.\n",
    "\n",
    "For `time<6`, there is almost zero correlation anywhere,\n",
    "so we should not trust `argmax`. Fallback to `time=6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216277ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_max_corr[:, :6] = xy_max_corr[:, [6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93bc240",
   "metadata": {},
   "source": [
    "Here is a plot of the paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465c02e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = freshfig(\"Trajectories of maxima of corr. fields\", figsize=(1.5, 1), rel=1)\n",
    "plotting.field(ax, np.zeros(model.shape), \"corr\", wells=True)\n",
    "for i, xy_path in enumerate(xy_max_corr):\n",
    "    color = dict(color=f\"C{i}\")\n",
    "    ax.plot(*xy_path.T, **color)\n",
    "    # Also mark start and end points\n",
    "    ax.plot(*xy_path[+0], \"x\", **color)\n",
    "    ax.plot(*xy_path[-1], \"o\", **color)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9a1e50-c5ff-4a60-869e-c71c71c23aaa",
   "metadata": {},
   "source": [
    "## Localization tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b17c0c9-928a-445e-96a1-04f1f51d3c28",
   "metadata": {},
   "source": [
    "It is technically challenging to translate/encode **all** of our prior knowledge into\n",
    "the computational form of an ensemble.  It is also computationally demanding, because\n",
    "a finite ensemble size, $N$, will contain sampling errors.  Thus, in principle, there\n",
    "is room to improve the performance of the ensemble methods by \"injecting\" more prior\n",
    "knowledge somehow, as an \"auxiliary\" technique.  A particularly effective way is\n",
    "**localization**, wherein we eliminate correlations (i.e. relationships) that we are\n",
    "\"pretty sure\" are *spurious*: merely due to sampling error, rather than indicative of\n",
    "an actual inter-dependence.\n",
    "\n",
    "Much can be said about the ad-hoc nature of most localization schemes.\n",
    "This is out of scope here.\n",
    "Furthermore, in particular in petroleum reservoir applications,\n",
    "configuring an effective localization setup can be very challenging.\n",
    "If successful, however, localization is unreasonably effective,\n",
    "allowing the use of much smaller ensemble sizes than one would think.\n",
    "\n",
    "In our simple case, it is sufficient to use distance-based localization.\n",
    "Far-away (remote) correlations will be dampened (\"tapered\").\n",
    "For the shape, we here use the \"bump function\" rather than the\n",
    "conventional (but unnecessarily complicated) \"Gaspari-Cohn\" piecewise polyomial\n",
    "function.  It is illustrated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d199c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = freshfig(\"Tapering ('bump') functions\", figsize=(1.5, .8), rel=1)\n",
    "dists = np.linspace(-1, 1, 1001)\n",
    "for sharpness in [.01, .1, 1, 10, 100, 1000]:\n",
    "    coeffs = loc.bump_function(dists, sharpness)\n",
    "    ax.plot(dists, coeffs, label=sharpness)\n",
    "ax.legend(title=\"sharpness\")\n",
    "ax.set_xlabel(\"Distance\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39418bd2-d70d-4678-952f-18c0409823a9",
   "metadata": {},
   "source": [
    "We will also need the distances, which we can pre-compute.\n",
    "As seen from `distances_to_obs` below, we will need the\n",
    "locations of each observation and each unknown parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d324019",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_obs = model.ind2xy(obs_inds)\n",
    "xy_prm = model.ind2xy(np.arange(model.Nxy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4271117d-4807-49d9-8015-37cf1fee5854",
   "metadata": {},
   "source": [
    "However, as we saw from the correlation dashboard, the localization should be\n",
    "time dependent. For example, it is tempting to say that remote-in-time (i.e. late)\n",
    "observations should have a larger area of impact,\n",
    "since they are integro-spatio-temperal functions (to use a fancy word) of the perm fields.\n",
    "We could achieve that by adding a time coordinate to `xy_obs` (setting it to 0 for `xy_prm`).\n",
    "However, the correlation dashboard does not really support this \"dilation\" theory,\n",
    "and we should be careful about growing the tapering mask.\n",
    "So instead, we simply replicate the same locations for each time instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb369b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_obs = np.tile(xy_obs, nTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e15e2",
   "metadata": {},
   "source": [
    "Actually, we can probably do a little better.  Recall from the correlation\n",
    "dashboard that the the correlation fields were clearly moving in time.\n",
    "Indeed, the maximum of the correlation to an observation was never even at\n",
    "the location of the well. Therefore, we will co-locate the correlation mask\n",
    "with these maxima, which we can achieve by computing distances to the maxima\n",
    "rather than to the wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df213238",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_obs = vect(xy_max_corr.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b89280-a11d-46f7-a72d-d9985533302d",
   "metadata": {},
   "source": [
    "Now we compute the distance between the parameters and the (argmax of the\n",
    "correlations with the) observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db28d2cf",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": []
   },
   "outputs": [],
   "source": [
    "distances_to_obs = loc.pairwise_distances(xy_prm.T, xy_obs.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe002110-2f32-4f28-9c86-a10f790c369e",
   "metadata": {},
   "source": [
    "The tapering function is similar to the covariance functions used in\n",
    "geostatistics (see Kriging, variograms), and indeed localization can be\n",
    "framed as a hybridisation of ensemble covariances with theoretical ones.\n",
    "However, the ideal tapering function does not generally equal the theoretical\n",
    "covariance function, but must instead be \"tuned\" for performance in the\n",
    "history match. Here we shall content ourselves simply with tuning a \"radius\"\n",
    "parameter.  Neverthless, tuning (wrt. history matching performance) is a\n",
    "breathtakingly costly proposition, requiring a great many synthetic\n",
    "experiments. This is made all the worse by the fact that it might have to be\n",
    "revisited later after some other factors have been tuned, or otherwise\n",
    "changed.\n",
    "\n",
    "Therefore, in lieu of such global tuning, we here undertake a study of the\n",
    "direct impact of the localization on the correlation fields. Fortunately, we\n",
    "can mostly just re-use the functionality from the above correlation\n",
    "dashboard, but now with some different controls; take a moment to study the\n",
    "function below, which generates the folowing plotted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7925aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_wells(N, t, well, localize, radi, sharp):\n",
    "    if not localize:\n",
    "        N = -1\n",
    "    C = utils.corr(perm.Prior[:N], prod.past.Prior[:N, t, well])\n",
    "    if localize:\n",
    "        dists = distances_to_obs[:, well + nProd*t]\n",
    "        c = loc.bump_function(dists/radi, 10**sharp)\n",
    "        C *= c\n",
    "        C[c < 1e-3] = np.nan\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e57e9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "corr_wells.controls = dict(\n",
    "    localize=False,\n",
    "    radi=(0.1, 5),\n",
    "    sharp=(-1.0, 1),\n",
    "    N=(2, N),\n",
    "    t=(0, nTime-1),\n",
    "    well=np.arange(nProd),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d10d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.field_console(corr_wells, \"corr\", \"Pre-perm vs well observation\", wells=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fdb50a-28a2-47ad-9fc8-38c08eaaf975",
   "metadata": {},
   "source": [
    "- Note that the `N` slider is only active when `localize` is *enabled*.\n",
    "  When localization is not enabled, then the full ensemble size is being used.\n",
    "- Set `N=20` and toggle `localize` on/off, while you play with different values of `radi`.\n",
    "  Try to find a value that makes the `localized` (small-ensemble) fields\n",
    "  resemble (as much as possible) the full-size ensemble fields.\n",
    "- The suggested value from the author is `0.8` (and sharpness $10^0$, i.e. 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa89b0b2",
   "metadata": {},
   "source": [
    "## Assimilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4ad082",
   "metadata": {},
   "source": [
    "### Ensemble update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39515800",
   "metadata": {},
   "source": [
    "Denote $\\mathbf{E}$ the ensemble matrix (whose columns are a sample from the prior),\n",
    "$\\mathcal{M}(\\mathbf{E})$ the observed ensemble,\n",
    "and $\\mathbf{D}$ be the observation perturbations.\n",
    "Let $\\mathbf{X}$ and $\\mathbf{Y}$ be the ensemble and the observed ensemble, respectively,\n",
    "but now with their (ensemble-) mean subtracted.\n",
    "Then the ensemble update can be written\n",
    "\n",
    "$$ \\mathbf{E}^a\n",
    "= \\mathbf{E}\n",
    "+ \\mathbf{X} \\mathbf{Y}^T\n",
    "\\big( \\mathbf{Y} \\mathbf{Y}^T + (N{-}1) \\mathbf{R} \\big)^{-1}\n",
    "\\big\\{ \\mathbf{y} \\mathbf{1}^T - [\\mathcal{M}(\\mathbf{E}) + \\mathbf{D}] \\big\\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a26ac6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def ens_update0(ens, obs_ens, observations, perturbs, obs_err_cov):\n",
    "    \"\"\"Compute the ensemble analysis (conditioning/Bayes) update.\"\"\"\n",
    "    X, _        = center(ens)\n",
    "    Y, _        = center(obs_ens)\n",
    "    perturbs, _ = center(perturbs, rescale=True)\n",
    "    obs_cov     = obs_err_cov*(len(Y)-1) + Y.T@Y\n",
    "    obs_pert    = perturbs @ sqrt(obs_err_cov)  # TODO: sqrtm if R non-diag\n",
    "    innovations = observations - (obs_ens + obs_pert)\n",
    "    KG          = sla.pinv(obs_cov) @ Y.T @ X\n",
    "    return ens + innovations @ KG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d1d995",
   "metadata": {},
   "source": [
    "Notes:\n",
    " - The formulae used by the code are transposed and reversed compared to the above.\n",
    "   [Rationale](https://nansencenter.github.io/DAPPER/dev_guide.html#conventions)\n",
    " - The perturbations are *input arguments* because we will want to re-use the same ones\n",
    "   when doing localization. It also enables exact reproducibility (see sanity check below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24394c94",
   "metadata": {},
   "source": [
    "### Bug check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e33be7f",
   "metadata": {},
   "source": [
    "It is very easy to introduce bugs.\n",
    "Fortunately, most can be eliminated with a few simple tests.\n",
    "\n",
    "For example, let us generate a case where both the unknown, $\\mathbf{x}$,\n",
    "and the observation error are (independently) $\\mathcal{N}(\\mathbf{0}, 2 \\mathbf{I})$,\n",
    "while the forward model is just the identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f8d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the prefix \"gg_\" stands for Gaussian-Gaussian\n",
    "gg_ndim = 3\n",
    "gg_prior = sqrt(2) * rnd.randn(1000, gg_ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89fd92",
   "metadata": {},
   "source": [
    "From theory, we know the posterior, $\\mathbf{x}|\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{y}/2, 1\\mathbf{I})$.\n",
    "Let us verify that the ensemble update computes this (up to sampling error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0489d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "gg_kwargs = dict(\n",
    "    ens          = gg_prior,\n",
    "    obs_ens      = gg_prior,\n",
    "    observations = 10*np.ones(gg_ndim),\n",
    "    obs_err_cov  = 2*np.eye(gg_ndim),\n",
    "    perturbs     = rnd.randn(*gg_prior.shape),\n",
    ")\n",
    "gg_postr = ens_update0(**gg_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72913bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(precision=1):\n",
    "    print(\"Posterior mean:\", np.mean(gg_postr, 0))\n",
    "    print(\"Posterior cov:\", np.cov(gg_postr.T), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282bab3f-6869-45ec-9c23-b4f667d8a64a",
   "metadata": {},
   "source": [
    "### Apply as smoother"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50b5cee-7ebd-4637-a62e-6a9664a7a583",
   "metadata": {},
   "source": [
    "#### Why not filtering?\n",
    "Before ensemble smoothers were used for history matching, it was though that\n",
    "*filtering*, rather than *smoothing*, should be used. As opposed to the (batch)\n",
    "ensemble smoothers, filters *sequentially* assimilate the time-series data,\n",
    "updating/conditioning both the saturation (i.e. state) fields and the permeability\n",
    "(i.e. parameter) fields. Some people might also call this a \"sequential\" smoother. In\n",
    "any case, this is problematic because the ensemble update is approximate, which not\n",
    "only causes statistical suboptimality, but also \"un-physical\" or \"non-realisable\"\n",
    "members -- a problem that gets exasperated by the simulator (manifesting as slow-down\n",
    "or crash, often due to convergence problems in the linear solver).  Moreover, the\n",
    "approximation (and hence the associated problems) only seem likely to worsen if using\n",
    "jointly-updated (rather than re-generated) state fields. This makes the\n",
    "parameter-only update of the (batch) smoothers appealing.\n",
    "Furthermore, the predominant uncertainty in history matching problems usually originates\n",
    "in the prior, rather than model error, reducing the potential for improvement by filtering.\n",
    "Finally, it is easier to formulate an iterative smoother than an iterative filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febefcb8",
   "metadata": {},
   "source": [
    "#### Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd3bb40",
   "metadata": {},
   "source": [
    "Our vector of unknowns is the pre-permeability.\n",
    "However, further below we will also apply the update to other unknowns\n",
    "(future saturation or productions). For brevity, we therefore collect the\n",
    "arguments that are common to all of the applications of this update.\n",
    "\n",
    "*PS: we could also pre-compute the matrices of the update that are common to\n",
    "all updates, thus saving time later. The fact that this is a possibility will\n",
    "not come as a surprise to readers familiar with state-vector augmentation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c75cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs0 = dict(\n",
    "    obs_ens      = vect(prod.past.Prior),\n",
    "    observations = vect(prod.past.Noisy),\n",
    "    perturbs     = rnd.randn(N, nProd*nTime),\n",
    "    obs_err_cov  = augmented_obs_error_cov,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec9efd",
   "metadata": {},
   "source": [
    "Thus the update is called as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f4964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm.ES = ens_update0(perm.Prior, **kwargs0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32827a9",
   "metadata": {},
   "source": [
    "#### Field plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ea2d5",
   "metadata": {},
   "source": [
    "Let's plot the updated ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e25718",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "plotting.fields(perm.ES, \"pperm\", \"ES (posterior)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e73f69c",
   "metadata": {},
   "source": [
    "We will see some more diagnostics later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828f893a-d67e-4758-a326-b7e81877a2e6",
   "metadata": {},
   "source": [
    "### With localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72088b9b-d55a-477c-8a78-d832f93e39cc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def ens_update0_loc(ens, obs_ens, observations, perturbs, obs_err_cov, domains, taper):\n",
    "    \"\"\"Perform local analysis/domain updates using `ens_update0`.\"\"\"\n",
    "    def local_analysis(ii):\n",
    "        \"\"\"Update for domain/batch `ii`.\"\"\"\n",
    "        # Get localization mask, coeffs\n",
    "        oBatch, tapering = taper(ii)\n",
    "        # Convert [range, slice, epsilon] to inds (for np.ix_)\n",
    "        oBatch = np.arange(len(observations))[oBatch]\n",
    "        # Update\n",
    "        if len(oBatch) == 0:\n",
    "            # no obs ==> no update\n",
    "            return ens[:, ii]\n",
    "        else:\n",
    "            c = sqrt(tapering)\n",
    "            return ens_update0(ens[:, ii],\n",
    "                               obs_ens[:, oBatch]*c,\n",
    "                               observations[oBatch]*c,\n",
    "                               perturbs[:, oBatch]*c,\n",
    "                               obs_err_cov[np.ix_(oBatch, oBatch)])\n",
    "\n",
    "    # Run -- could use multiprocessing here (replace `map` by `mp`),\n",
    "    # but in our case the overhead means that it's not worth it.\n",
    "    EE = map(local_analysis, domains)\n",
    "\n",
    "    # Write to ensemble matrix. NB: don't re-use `ens`!\n",
    "    Ea = np.empty_like(ens)\n",
    "    for ii, Eii in zip(domains, EE):\n",
    "        Ea[:, ii] = Eii\n",
    "\n",
    "    return Ea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f74d45",
   "metadata": {},
   "source": [
    "The form of the localization used in the above code is \"local/domain analysis\".\n",
    "Note that it sequentially processing batches (subsets/domains)\n",
    "of the vector of unknowns (actually, ideally, we'd iterate over each single element,\n",
    "but that is usually computationally inefficient).\n",
    "\n",
    "The localisation setup (`taper`) must return a mask or list of indices\n",
    "that select the observations near the local domain `ii`,\n",
    "and the corresponding tapering coefficients.\n",
    "For example, consider this setup,\n",
    "which makes the update process each local domain entirely independently,\n",
    "*assuming an identity forward model, i.e. that `obs := prm + noise`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d0e80",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def full_localization(batch_inds):\n",
    "    return batch_inds, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8422301-850a-43f9-879d-4754547a29ca",
   "metadata": {},
   "source": [
    "#### Bug check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf6ec4f",
   "metadata": {},
   "source": [
    "Again, the (localized) method should yield the correct posterior,\n",
    "up to some sampling error. However, thanks to `full_localization`,\n",
    "this error should be smaller than in our bug check for `ens_update0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f086082e-5207-428e-8e36-76fe78e5fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gg_postr = ens_update0_loc(**gg_kwargs, domains=np.c_[:gg_ndim], taper=full_localization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9271f668",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "with np.printoptions(precision=1):\n",
    "    print(\"Posterior mean:\", np.mean(gg_postr, 0))\n",
    "    print(\"Posterior cov:\", np.cov(gg_postr.T), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c3568-7a4e-4b74-8b7b-408bdc95aaec",
   "metadata": {},
   "source": [
    "#### Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5cba8c-4c1e-49f8-afd1-30a559d14bc7",
   "metadata": {},
   "source": [
    "Now consider the following setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef2f2e6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def no_localization(batch_inds):\n",
    "    return ..., 1  # ellipsis (...) means \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60991693",
   "metadata": {},
   "source": [
    "Hopefully, using this should output the same ensemble (up to *numerical* error)\n",
    "as `ens_update0`. Let us verify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f26a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = ens_update0_loc(perm.Prior, **kwargs0, domains=[...], taper=no_localization)\n",
    "print(\"Reproduces global analysis?\", np.allclose(tmp, perm.ES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f395be7a",
   "metadata": {},
   "source": [
    "*PS: with no localization, it should not matter how the domain is partitioned.\n",
    "For example, try `domains=np.arange(model.Nxy).reshape(some_integer, -1)`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646c442a",
   "metadata": {},
   "source": [
    "#### Configuration for the history matching problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84a6f8c",
   "metadata": {},
   "source": [
    "Now let us define the local domains for the permeability field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020bcc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = loc.rectangular_partitioning(model.shape, (2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b329227e-db9e-4b9a-a528-48698a23a16b",
   "metadata": {},
   "source": [
    "We can illustrate the partitioning by filling each domain by a random color.\n",
    "This should produce a patchwork of rectangles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff152e6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "colors = rnd.choice(len(domains), len(domains), False)\n",
    "Z = np.zeros(model.shape)\n",
    "for d, c in zip(domains, colors):\n",
    "    Z[tuple(model.ind2sub(d))] = c\n",
    "fig, ax = freshfig(\"Computing domains\", figsize=(1, .5), rel=1)\n",
    "ax.imshow(Z, cmap=\"tab20\", aspect=.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5185cf0-6eba-4349-8f81-c4d5133c978e",
   "metadata": {},
   "source": [
    "The tapering will be a function of the batch's mean distance to the observations.\n",
    "The default `radius` and `sharpness` are the ones we found to be the most\n",
    "promising from the above correlation study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d5b0f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def localization_setup(batch, radius=0.8, sharpness=1):\n",
    "    dists = distances_to_obs[batch].mean(axis=0)\n",
    "    obs_coeffs = loc.bump_function(dists/radius, sharpness)\n",
    "    obs_mask = obs_coeffs > 1e-3\n",
    "    return obs_mask, obs_coeffs[obs_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8d37bb-0269-4ccf-9b06-323df09fc522",
   "metadata": {},
   "source": [
    "#### Apply as smoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caf848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm.LES = ens_update0_loc(perm.Prior, **kwargs0,\n",
    "                           domains=domains, taper=localization_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13971ae7",
   "metadata": {},
   "source": [
    "Again, we plot some updated/posterior fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9a4424",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "plotting.fields(perm.LES, \"pperm\", \"LES (posterior)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb643fec",
   "metadata": {},
   "source": [
    "### Iterative ensemble smoother"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63b75b",
   "metadata": {},
   "source": [
    "#### Why iterate?\n",
    "Due to non-linearity of the forward model, the likelihood is non-Gaussian, and\n",
    "ensemble methods do not compute the true posterior (even with infinite `N`).  Still,\n",
    "after the update, it may be expected that the estimate of the sensitivity (of the\n",
    "model to the observations) has improved. Thus, it makes sense to retry the update\n",
    "(starting from the prior again, so as not to over-condition/use the data), but this\n",
    "time with the improved sensitivity estimate.  This cycle can then be repeated\n",
    "indefinitely.\n",
    "\n",
    "Caution: the meaning of \"improvement\" of the sensitivity estimate is not well defined.\n",
    "It is known that ensemble sensitivities estimate the *average* sensitivities\n",
    "([[Raa19]](#Raa19)); however, it does not seem possible to prove (with generality)\n",
    "that the average defined by the (iteratively approximated) posterior is better suited\n",
    "than that of the prior, as neither one will yield the correct posterior.\n",
    "Nevertheless, when accompanied by sufficient hand-waving, most people will feel\n",
    "convinced by the above argument, or something similar.\n",
    "\n",
    "Another perspective is that the iterations *might* manage to find the\n",
    "mode of the posterior, i.e. perform maximum-a-posteriori (MAP) estimation.\n",
    "This perspective comes from weather forecasting and their \"variational\"\n",
    "methods, as well as classical (extended, iterative) Kalman filtering.\n",
    "However, this perspective is more of a first-order approximation\n",
    "to the fully Bayesian uncertainty quantification approximated by ensemble methods.\n",
    "\n",
    "In any case, empricial evidence leave little room to doubt that iterations\n",
    "yield improved estiamtion accuracy, albeit a the cost of (linearly) more\n",
    "computational effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f08f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IES_analysis(w, T, Y, dy):\n",
    "    \"\"\"Compute the ensemble analysis.\"\"\"\n",
    "    N = len(Y)\n",
    "    Y0       = sla.pinv(T) @ Y        # \"De-condition\"\n",
    "    V, s, UT = utils.svd0(Y0)         # Decompose\n",
    "    Cowp     = utils.pows(V, utils.pad0(s**2, N) + N-1)\n",
    "    Cow1     = Cowp(-1.0)             # Posterior cov of w\n",
    "    grad     = Y0@dy - w*(N-1)        # Cost function gradient\n",
    "    dw       = grad@Cow1              # Gauss-Newton step\n",
    "    T        = Cowp(-.5) * sqrt(N-1)  # Transform matrix\n",
    "    return dw, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09afec28",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def IES(ensemble, observations, obs_err_cov, stepsize=1, nIter=10, wtol=1e-4):\n",
    "    \"\"\"Iterative ensemble smoother.\"\"\"\n",
    "    E = ensemble\n",
    "    y = observations\n",
    "    N = len(E)\n",
    "    N1 = N - 1\n",
    "    Rm12T = np.diag(sqrt(1/np.diag(obs_err_cov)))  # TODO?\n",
    "\n",
    "    # Init\n",
    "    stat = Dict(dw=[], rmse=[], stepsize=[],\n",
    "                obj=Dict(lklhd=[], prior=[], postr=[]))\n",
    "\n",
    "    # Init ensemble decomposition.\n",
    "    X0, x0 = center(E)    # Decompose ensemble.\n",
    "    w      = np.zeros(N)  # Control vector for the mean state.\n",
    "    T      = np.eye(N)    # Anomalies transform matrix.\n",
    "\n",
    "    for itr in progbar(range(nIter), desc=\"Iter\"):\n",
    "        # Compute rmse (vs. supposedly unknown Truth)\n",
    "        stat.rmse += [utils.norm(E.mean(0) - perm.Truth)]\n",
    "\n",
    "        # Forecast.\n",
    "        _, Eo = ens_run(forward_model, wsat.init.Prior, E, leave=False)\n",
    "        Eo = vect(Eo)\n",
    "\n",
    "        # Prepare analysis.\n",
    "        Y, xo  = center(Eo)         # Get anomalies, mean.\n",
    "        dy     = (y - xo) @ Rm12T   # Transform obs space.\n",
    "        Y      = Y        @ Rm12T   # Transform obs space.\n",
    "\n",
    "        # Diagnostics\n",
    "        stat.obj.prior += [w@w * N1]\n",
    "        stat.obj.lklhd += [dy@dy]\n",
    "        stat.obj.postr += [stat.obj.prior[-1] + stat.obj.lklhd[-1]]\n",
    "\n",
    "        reject_step = itr > 0 and stat.obj.postr[itr] > np.min(stat.obj.postr)\n",
    "        if reject_step:\n",
    "            # Restore prev. ensemble, lower stepsize\n",
    "            stepsize   /= 10\n",
    "            w, T        = old  # noqa\n",
    "        else:\n",
    "            # Store current ensemble, boost stepsize\n",
    "            old         = w, T\n",
    "            stepsize   *= 2\n",
    "            stepsize    = min(1, stepsize)\n",
    "\n",
    "            dw, T = IES_analysis(w, T, Y, dy)\n",
    "\n",
    "        stat.dw += [dw@dw / N]\n",
    "        stat.stepsize += [stepsize]\n",
    "\n",
    "        # Step\n",
    "        w = w + stepsize*dw\n",
    "        E = x0 + (w + T)@X0\n",
    "\n",
    "        if stepsize * np.sqrt(dw@dw/N) < wtol:\n",
    "            break\n",
    "\n",
    "    # The last step must be discarded,\n",
    "    # because it cannot be validated without re-running the model.\n",
    "    w, T = old\n",
    "    E = x0 + (w+T)@X0\n",
    "\n",
    "    return E, stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaef507",
   "metadata": {},
   "source": [
    "#### Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff1a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargsI = dict(\n",
    "    observations = vect(prod.past.Noisy),\n",
    "    obs_err_cov  = augmented_obs_error_cov,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f3e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm.IES, diagnostics = IES(perm.Prior, **kwargsI, stepsize=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb276134",
   "metadata": {},
   "source": [
    "#### Field plots\n",
    "Let's plot the updated, initial ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2456f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.fields(perm.IES, \"pperm\", \"IES (posterior)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd91c7",
   "metadata": {},
   "source": [
    "The following plots the cost function(s) together with the error compared to the true\n",
    "(pre-)perm field as a function of the iteration number. Note that the relationship\n",
    "between the (total, i.e. posterior) cost function  and the RMSE is not necessarily\n",
    "monotonic. Re-running the experiments with a different seed is instructive. It may be\n",
    "observed that the iterations are not always very successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa73641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = freshfig(\"IES Objective function\")\n",
    "ls = dict(postr=\"-\", prior=\":\", lklhd=\"--\")\n",
    "for name, J in diagnostics.obj.items():\n",
    "    ax.plot(np.sqrt(J), color=\"b\", ls=ls[name], label=name)\n",
    "ax.set_xlabel(\"iteration\")\n",
    "ax.set_ylabel(\"RMS mismatch\", color=\"b\")\n",
    "ax.tick_params(axis='y', labelcolor=\"b\")\n",
    "ax.legend()\n",
    "ax2 = ax.twinx()  # axis for rmse\n",
    "ax2.set_ylabel('RMS error', color=\"r\")\n",
    "ax2.plot(diagnostics.rmse, color=\"r\")\n",
    "ax2.tick_params(axis='y', labelcolor=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e5dae",
   "metadata": {},
   "source": [
    "## Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b12ef4",
   "metadata": {},
   "source": [
    "In terms of root-mean-square error (RMSE), the ES is expected to improve on the prior.\n",
    "The \"expectation\" wording indicates that this is true on average, but not always. To\n",
    "be specific, it means that it is guaranteed to hold true if the RMSE is calculated for\n",
    "infinitely many experiments (each time simulating a new synthetic truth and\n",
    "observations from the prior). The reason for this is that the ES uses the Kalman\n",
    "update, which is the BLUE (best linear unbiased estimate), and \"best\" means that the\n",
    "variance must get reduced. However, note that this requires the ensemble to be\n",
    "infinitely big, which it most certainly is not in our case. Therefore, we do not need\n",
    "to be very unlucky to observe that the RMSE has actually increased. Despite this, as\n",
    "we will see later, the data match might yield a different conclusions concerning the\n",
    "utility of the update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e1f3d8",
   "metadata": {},
   "source": [
    "### Means vs. True field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97040b4",
   "metadata": {},
   "source": [
    "#### RMS summary\n",
    "RMS stands for \"root-mean-square(d)\" and is a summary measure for deviations.\n",
    "With ensemble methods, it is (typically, and in this case study) applied\n",
    "to the deviation from the **ensemble mean**, whence the trailing `M` in `RMSMs` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8d97f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stats vs. true field\\n\")\n",
    "utils.RMSMs(perm, ref=\"Truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada437f5",
   "metadata": {},
   "source": [
    "#### Field plots\n",
    "Let's plot mean fields.\n",
    "\n",
    "NB: Caution! Mean fields are liable to smoother than the truth. This is a phenomenon\n",
    "familiar from geostatistics (e.g. Kriging). As such, their importance must not be\n",
    "overstated (they're just one estimator out of many). Instead, whenever a decision is\n",
    "to be made, all of the members should be included in the decision-making process. This\n",
    "does not mean that you must eyeball each field, but that decision analyses should be\n",
    "based on expected values with respect to ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85fd4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_means = Dict({k: perm[k].mean(axis=0) for k in perm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e54053",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.fields(perm_means, \"pperm\", \"Means\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d542ba5",
   "metadata": {},
   "source": [
    "### Means vs. Data mismatch (past production)\n",
    "In synthetic experiments such as this one, is is instructive to computing the \"error\":\n",
    "the difference/mismatch of the (supposedly) unknown parameters and the truth.  Of\n",
    "course, in real life, the truth is not known.  Moreover, at the end of the day, we\n",
    "mainly care about production rates and saturations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03adbbc5",
   "metadata": {},
   "source": [
    "#### Re-run\n",
    "Therefore, let us now compute the \"residual\" (i.e. the mismatch between\n",
    "predicted and true *observations*), which we get from the predicted\n",
    "production \"profiles\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for methd in perm:\n",
    "    if methd not in prod.past:\n",
    "        print(methd, \":\")\n",
    "        s, p = ens_run(forward_model, wsat.init.Prior, perm[methd])\n",
    "        wsat.past[methd], prod.past[methd] = s, p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbbe564",
   "metadata": {},
   "source": [
    "The ES can be applied to any un-conditioned ensemble (not just the permeabilities).\n",
    "A particularly interesting case is applying it to the prior's production predictions.\n",
    "This provides another posterior approximation of the production history\n",
    "-- one which doesn't require running the model again\n",
    "(in contrast to what we did for `prod.past.(I)ES` immediately above).\n",
    "Since it requires 0 iterations, let's call this \"ES0\". Let us try that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ad8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod.past.ES0 = vect(ens_update0(vect(prod.past.Prior), **kwargs0), undo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b438bae",
   "metadata": {},
   "source": [
    "#### Production plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e473abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.productions(prod.past, \"Past\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc38dbb",
   "metadata": {},
   "source": [
    "##### Comment on prior\n",
    "Note that the prior \"surrounds\" the data. This the likely situation in our synthetic\n",
    "case, where the truth was generated by the same random draw process as the ensemble.\n",
    "\n",
    "In practice, this is often not the case. If so, you might want to go back to your\n",
    "geologists and tell them that something is amiss. You should then produce a revised\n",
    "prior with better properties.\n",
    "\n",
    "Note: the above instructions sound like statistical heresy. We are using the data\n",
    "twice over (on the prior, and later to update/condition the prior). However, this is\n",
    "justified to the extent that prior information is difficult to quantify and encode.\n",
    "Too much prior adaptation, however, and you risk overfitting! Indeed, it is a delicate\n",
    "matter. It is likely best resolved by only revising coarse features of the prior,\n",
    "and increasing its uncertainty rather than trying to adjust its mean (bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22fc79d",
   "metadata": {},
   "source": [
    "##### Comment on posterior\n",
    "If the assumptions (statistical indistinguishability, Gaussianity) are not too far\n",
    "off, then the ensemble posteriors (`ES`, `LES`, `IES`, `ES0`)\n",
    "should also surround the data, but with a tighter fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a924749d",
   "metadata": {},
   "source": [
    "#### RMS summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3a465",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stats vs. past production (i.e. NOISY observations)\\n\")\n",
    "utils.RMSMs(prod.past, ref=\"Noisy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a13e5a1",
   "metadata": {},
   "source": [
    "Note that, here, the \"err\" is comptuted vs. the observations,\n",
    "not the (supposedly unknown) truth. In any case,\n",
    "the `rms err` for any of the (approximate) posteriors should be lower than\n",
    "the `rms err` of the `Prior`, although for very small $N$ this may (spuriously) fail.\n",
    "Moreover, in the linear-Gaussian, infinite-$N$ case, the `rms err` of the posteriors\n",
    "should also be lower than that of the `Truth`\n",
    "(whose `rms err` approximates the std. dev. of the obs. noise).\n",
    "Evidently these assumptions are far from valid,\n",
    "since none of the approximate posteriors (i.e. methods) achieve this\n",
    "low magnitude of error.\n",
    "\n",
    "Note that the error of `ES0` is very low. As we shall see, however,\n",
    "this \"method\" is very poor at prediction (in this nonlinear case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26326a4",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51022f58",
   "metadata": {},
   "source": [
    "We now prediction the future by forecasting from the current (present-time) ensembles.\n",
    "\n",
    "Note that we must use the current saturation in the \"restart\" for the predictive\n",
    "simulations. Since the estimates of the current saturation depend on the assumed\n",
    "permeability field, these estimates are also \"posterior\", and depend on the\n",
    "conditioning method used. For convenience, we first extract the slice of the current\n",
    "saturation fields (which is really the only one we make use of among those of the\n",
    "past), and plot the mean fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e345b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsat.curnt = Dict({k: v[..., -1, :] for k, v in wsat.past.items()})\n",
    "wsat_means = Dict({k: np.atleast_2d(v).mean(axis=0) for k, v in wsat.curnt.items()})\n",
    "plotting.fields(wsat_means, \"oil\", \"Means\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4334c30",
   "metadata": {},
   "source": [
    "#### Run\n",
    "Now we predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fce795",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Future/prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d1cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsat.futr.Truth = simulator.recurse(model.time_stepper(dt), nTime, wsat.curnt.Truth)\n",
    "prod.futr.Truth = np.array([obs_model(x) for x in wsat.futr.Truth[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for methd in perm:\n",
    "    if methd not in prod.futr:\n",
    "        print(methd, \":\")\n",
    "        s, p = ens_run(forward_model, wsat.curnt[methd], perm[methd])\n",
    "        wsat.futr[methd], prod.futr[methd] = s, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8f8c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod.futr.ES0 = vect(ens_update0(vect(prod.futr.Prior), **kwargs0), undo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7816056a",
   "metadata": {},
   "source": [
    "#### Production plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f1f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.productions(prod.futr, \"Future\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85969a4a",
   "metadata": {},
   "source": [
    "#### RMS summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc23d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stats vs. (supposedly unknown) future production\\n\")\n",
    "utils.RMSMs(prod.futr, ref=\"Truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eca8e2",
   "metadata": {},
   "source": [
    "## Final comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5871943d",
   "metadata": {},
   "source": [
    "It is instructive to run this notebook/script again, but with a different random seed.\n",
    "This will yield a different truth, and noisy production data, and so a new\n",
    "case/problem, which may be more, or less, difficult.\n",
    "\n",
    "Another alternative is to only re-run the notebook cells starting from where the prior\n",
    "was sampled. Thus, the truth and observations will not change, yet because the prior\n",
    "sample will change, the results will change. If this change is significant (which can\n",
    "only be asserted by re-running the experiments several times), then you cannot have\n",
    "much confidence in your result. In order to fix this, you must increase the ensemble\n",
    "size (to reduce sampling error), or try to tune parameters such as the\n",
    "localization radius (or more generally, improve your localization implementation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f69c2",
   "metadata": {},
   "source": [
    "Either way, re-running the synthetic experiments and checking that your setup and tuning\n",
    "produces resonably improved results will give you confidence in their generalizability;\n",
    "as such it it similar in its aim to statistical cross-validation.\n",
    "For this reason, synthetic experiments should also be applied in real applications!\n",
    "The fact that the real truth is unknown does not prevent you from testing your setup\n",
    "with a synthetic truth, sampled from the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8eca61",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b83104",
   "metadata": {},
   "source": [
    "<a id=\"Jaz70\">[Jaz70]</a>: Jazwinski, A. H. 1970. *Stochastic Processes and Filtering Theory*. Vol. 63. Academic Press.\n",
    "\n",
    "<a id=\"Raa19\">[Raa19]</a>: Raanes, Patrick Nima, Andreas Størksen Stordal, and Geir Evensen. 2019. “Revising the Stochastic Iterative Ensemble Smoother.” *Nonlinear Processes in Geophysics* 26 (3): 325–38.  https://doi.org/10.5194/npg-26-325-2019."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "auto:light,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
